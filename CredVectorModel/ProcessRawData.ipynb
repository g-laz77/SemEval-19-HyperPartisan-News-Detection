{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.sax\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ArticleParser(xml.sax.ContentHandler):\n",
    "    def __init__(self, folder):\n",
    "        self.folder = folder\n",
    "        self.CurrentData = \"\"\n",
    "        self.title = \"\"\n",
    "        self.id = \"\"\n",
    "        self.p = \"\"\n",
    "        self.published_at = \"\"\n",
    "        self.content = []\n",
    "        self.urls = []\n",
    "        \n",
    "    def startElement(self, tag, attributes):\n",
    "        self.CurrentData = tag\n",
    "        if tag == \"article\":\n",
    "            try: self.published_at = attributes['published-at']\n",
    "            except: self.published_at = None\n",
    "            self.id = attributes['id']\n",
    "            self.title = attributes['title']\n",
    "        if tag in [\"p\", \"a\"]:\n",
    "            self.content.append(\"\")\n",
    "        if tag == \"a\":\n",
    "            try:\n",
    "                self.urls.append(attributes['href'].split('/')[2])\n",
    "                words = attributes['href'].split('/')[2].split('.')\n",
    "                url = \"\"\n",
    "                if not words[0] == 'www': url += words[0]\n",
    "                else: url += words[1]\n",
    "                self.content.append(' & ' +  url + ' & ')\n",
    "            except: pass\n",
    "            \n",
    "            \n",
    "    def endElement(self, tag):\n",
    "        if tag == 'article':\n",
    "            with open(self.folder + str(self.id), 'w+') as f:\n",
    "                f.write(self.title)\n",
    "                f.write('\\n\\n')\n",
    "              #  for url in self.urls:\n",
    "               #     f.write(url)\n",
    "                #    f.write('\\n')\n",
    "                f.write('\\n\\n')\n",
    "                f.writelines(self.content)\n",
    "            self.CurrentData = \"\"\n",
    "            self.title = \"\"\n",
    "            self.id = \"\"\n",
    "            self.p = \"\"\n",
    "            self.published_at = \"\"\n",
    "            self.content = []\n",
    "            self.urls = []\n",
    "            \n",
    "    def characters(self, content):\n",
    "        if self.CurrentData in [\"p\", \"a\"]:\n",
    "            self.content[-1] += content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = xml.sax.make_parser()\n",
    "parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n",
    "Handler = ArticleParser('dataset/training/')\n",
    "parser.setContentHandler(Handler)\n",
    "#shutil.rmtree('dataset/training')\n",
    "#print('deleted')\n",
    "#os.mkdir('dataset/training')\n",
    "parser.parse('dataset/articles-training-20180831.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = xml.sax.make_parser()\n",
    "parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n",
    "Handler = ArticleParser('dataset/validation/')\n",
    "parser.setContentHandler(Handler)\n",
    "#shutil.rmtree('dataset/validation')\n",
    "#print('deleted')\n",
    "#os.mkdir('dataset/validation/')\n",
    "parser.parse('dataset/articles-validation-20180831.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GroundTruthHandler(xml.sax.ContentHandler):\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        \n",
    "    def startElement(self, tag, attributes):\n",
    "        file = self.file\n",
    "        if tag == 'article':\n",
    "            file.write(attributes['id'])\n",
    "            file.write(',')\n",
    "            file.write(attributes['hyperpartisan'])\n",
    "            file.write(',')\n",
    "            file.write(attributes['bias'])\n",
    "            file.write(',')\n",
    "            file.write(attributes['url'].split('/')[2])\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open('training_labels', 'w+')\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n",
    "Handler = GroundTruthHandler(file)\n",
    "parser.setContentHandler(Handler)\n",
    "parser.parse('dataset/ground-truth-training-20180831.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open('validation_labels', 'w+')\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n",
    "Handler = GroundTruthHandler(file)\n",
    "parser.setContentHandler(Handler)\n",
    "parser.parse('dataset/ground-truth-validation-20180831.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ramkishore.s/py3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ramkishore.s/py3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ramkishore.s/py3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SENTENCE SPLITTER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|Mt)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov|me|edu)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = text.replace('?', ' ')   # data cleaning, The given XML file has many ? for unknown or special characters. \n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\".\")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD GLOVE VECTORS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 400000 400000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "WORDS = []\n",
    "WORD_ID_MAPPING = {}\n",
    "VECTORS = []\n",
    "id = 0\n",
    "dims = 100\n",
    "glove_file = '/home/ramkishore.s/glove/glove.6B.100d.txt'\n",
    "with open(glove_file) as f:\n",
    "    for l in f:\n",
    "        line = l.split()\n",
    "        word = line[0]\n",
    "        WORDS.append(word)\n",
    "        WORD_ID_MAPPING[word] , id = id, id + 1\n",
    "        vect = np.array(line[1:]).astype(np.float)\n",
    "        VECTORS.append(vect)\n",
    "        \n",
    "print(len(WORDS), len(WORD_ID_MAPPING), len(VECTORS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UNK = '@@@UNK@@@'\n",
    "WORDS.append(UNK)\n",
    "WORD_ID_MAPPING[UNK] = len(WORDS) - 1\n",
    "VECTORS.extend(np.random.randn(1, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROCESSING DATA FOR DEEP LEARNING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FOLDER = 'dataset/training/'\n",
    "VALIDATION_FOLDER = 'dataset/validation/'\n",
    "TRAIN_FILES = os.listdir(TRAIN_FOLDER)\n",
    "VALIDATION_FILES = os.listdir(VALIDATION_FOLDER)\n",
    "START = '<start>'\n",
    "END = '<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORDS.append(START)\n",
    "WORDS.append(END)\n",
    "WORD_ID_MAPPING[START], WORD_ID_MAPPING[END] = len(WORDS) - 2, len(WORDS) - 1\n",
    "VECTORS.extend(np.random.randn(2, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADD PADDING VECTOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VECTORS.extend(np.zeros(1, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LINK_PATTERN = re.compile('\\&.*\\&')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "URLS = {}\n",
    "URL_ID_COUNT = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION TO TOKENIZE FILE, EXTRACT CITED URLS AND TITLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_file(file):\n",
    "    f = open(file).read()\n",
    "    lines = split_into_sentences(f)\n",
    "    lines = [line.lower() for line in lines]\n",
    "    urls, words = [], []\n",
    "    title = nltk.word_tokenize(lines[0])\n",
    "    for line in lines[1:]:\n",
    "        if len(line) < 2: continue\n",
    "        links = LINK_PATTERN.findall(line)\n",
    "        if len(links) > 0: \n",
    "            try:\n",
    "                urls.append(links[0].split()[1])\n",
    "                line.replace(links[0], ' ')\n",
    "            except: \n",
    "                urls.append('$')\n",
    "        else: \n",
    "            urls.append('$')\n",
    "        words.append([START] + nltk.word_tokenize(line) + [END])\n",
    "    return title, urls, words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION TO MAP TOKENS TO NUMBERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_vectors(title, urls, lines):\n",
    "    global URL_ID_COUNT, URLS, WORD_ID_MAPPING\n",
    "    \n",
    "    url_ids, line_ids = [], []\n",
    "    for url in urls:\n",
    "        if url not in URLS:\n",
    "            URLS[url], URL_ID_COUNT = URL_ID_COUNT, URL_ID_COUNT + 1\n",
    "        url_ids.append(URLS[url])\n",
    "        \n",
    "    for line in lines:\n",
    "        line_ids.append([])\n",
    "        for word in line:\n",
    "            if word in WORD_ID_MAPPING:\n",
    "                line_ids[-1].append(WORD_ID_MAPPING[word])      # word in vocabulary\n",
    "            else:\n",
    "                line_ids[-1].append(WORD_ID_MAPPING[UNK])       # unknown word\n",
    "\n",
    "    title_ids = [WORD_ID_MAPPING[w] if w in WORD_ID_MAPPING else WORD_ID_MAPPING[UNK] for w in title]\n",
    "    \n",
    "    return  title_ids, url_ids, line_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROCESSING OUTPUTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTPUTS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_outputs(file, dict_):\n",
    "    lines = open(file).readlines()\n",
    "    for line in lines:\n",
    "        words = line.split(',')\n",
    "        output = []\n",
    "        if words[1] == 'true':\n",
    "            output.append(True)\n",
    "        else:\n",
    "            output.append(False)\n",
    "        output.append(words[2])\n",
    "        dict_[words[0]] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_outputs('dataset/training_labels', OUTPUTS)\n",
    "load_outputs('dataset/validation_labels', OUTPUTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROCESS DATA AND STORE IN PKL FORMAT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import _pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(folder, per_file=100000, name='training'):              # per_file -> no of files to store in one pkl file\n",
    "    # process each file\n",
    "    # convert them to vectors\n",
    "    # store them in pickle/json periodically\n",
    "    FINAL_DATA = []\n",
    "    count = 0\n",
    "    file_id  = 0\n",
    "    for file in os.listdir(folder):\n",
    "        print(count + file_id * per_file, end='\\r')\n",
    "        title, urls, words = process_file(folder + file)\n",
    "        title_ids, url_ids, line_ids = convert_to_vectors(title, urls, words)\n",
    "        FINAL_DATA.append([title_ids, url_ids, line_ids] + OUTPUTS[file])\n",
    "        count += 1\n",
    "        if count == per_file -1:\n",
    "            pickle.dump(FINAL_DATA, open(name + \"_\" + str(file_id), 'wb+'))\n",
    "            FINAL_DATA = []\n",
    "            file_id = file_id + 1\n",
    "            count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800007\r"
     ]
    }
   ],
   "source": [
    "process(TRAIN_FOLDER, name='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200001\r"
     ]
    }
   ],
   "source": [
    "process(VALIDATION_FOLDER, name='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STORE**\n",
    "* Word Vectors\n",
    "* Word Mappings\n",
    "* Url Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(VECTORS, open('glove_100d_vectors', 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(WORD_ID_MAPPING, open('word_mappings', 'wb+'))\n",
    "pickle.dump(URLS, open('urls', 'wb+'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
