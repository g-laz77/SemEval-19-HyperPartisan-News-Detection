{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import *\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcc_matmul(repr_vec, bias, weight, nonlinearity='sigmoid'):\n",
    "    s = torch.addmv(bias, weight, repr_vec)\n",
    "    if nonlinearity == 'sigmoid':\n",
    "        s = torch.sigmoid(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_matmul_bias(seq, weight, bias, nonlinearity=''):\n",
    "    s = None\n",
    "    bias_dim = bias.size()\n",
    "    for i in range(seq.size(0)):\n",
    "        _s = torch.mm(seq[i], weight) \n",
    "        _s_bias = _s + bias.expand(bias_dim[0], _s.size()[0]).transpose(0,1)\n",
    "        if(nonlinearity=='tanh'):\n",
    "            _s_bias = torch.tanh(_s_bias)\n",
    "        _s_bias = _s_bias.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s_bias\n",
    "        else:\n",
    "            s = torch.cat((s,_s_bias),0)\n",
    "    return s.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_matmul(seq, weight, nonlinearity=''):\n",
    "    s = None\n",
    "    for i in range(seq.size(0)):\n",
    "        _s = torch.mm(seq[i], weight)\n",
    "        if(nonlinearity=='tanh'):\n",
    "            _s = torch.tanh(_s)\n",
    "        _s = _s.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)\n",
    "    return s.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_mul(rnn_outputs, att_weights):\n",
    "    attn_vectors = None\n",
    "    for i in range(rnn_outputs.size(0)):\n",
    "        h_i = rnn_outputs[i]\n",
    "        a_i = att_weights[i].unsqueeze(1).expand_as(h_i)\n",
    "        h_i = a_i * h_i\n",
    "        h_i = h_i.unsqueeze(0)\n",
    "        if(attn_vectors is None):\n",
    "            attn_vectors = h_i\n",
    "        else:\n",
    "            attn_vectors = torch.cat((attn_vectors,h_i),0)\n",
    "    return torch.sum(attn_vectors, 0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEncoderRNN(nn.Module): # Encoder GRU which give hidden state representations\n",
    "    def __init__(self, batch_size, hidden_size, embed_size, n_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.input_embed_size = embed_size\n",
    "        self.embedding = nn.Embedding(batch_size, input_embed_size)\n",
    "        self.gru = nn.GRU(input_embed_size, hidden_size,n_layers, bidirectional=True)\n",
    "\n",
    "    def forward(self, src, hidden):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, _ = self.gru(embedded, hidden)\n",
    "        outputs = (outputs[:, :, :self.hidden_size] +\n",
    "                   outputs[:, :, self.hidden_size:])\n",
    "        return outputs\n",
    "    \n",
    "class WordAttention(nn.Module): # Attention mechanism for word hidden states\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.weight_W_word = nn.Parameter(torch.Tensor(hidden_size * 2, hidden_size * 2))\n",
    "        self.bias_word = nn.Parameter(torch.Tensor(hidden_size*2,1))\n",
    "        self.weight_proj_word = nn.Parameter(torch.Tensor(hidden_size*2, 1))\n",
    "        self.softmax_word = nn.Softmax()\n",
    "        self.weight_W_word.data.uniform_(-0.1, 0.1)\n",
    "        self.weight_proj_word.data.uniform_(-0.1,0.1)\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        # encoder_outputs = encoder_outputs.transpose(0, 1) # [B*T*H]\n",
    "        word_squish = batch_matmul_bias(encoder_outputs, self.weight_W_word,self.bias_word, nonlinearity='tanh')\n",
    "        word_attn = batch_matmul(word_squish, self.weight_proj_word)\n",
    "        word_attn_norm = self.softmax_word(word_attn.transpose(1,0))\n",
    "        word_attn_vectors = attention_mul(encoder_outputs, word_attn_norm.transpose(1,0))\n",
    "        return word_attn_vectors, word_attn_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentEncoderRNN(nn.Module): # Encoder GRU which give hidden state representations\n",
    "    def __init__(self, batch_size, sent_hidden_size, word_hidden_size):\n",
    "        super().__init__()\n",
    "        self.sent_hidden_size = sent_hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.word_hidden_size = word_hidden_size\n",
    "        self.n_classes = n_classes\n",
    "        self.gru = nn.GRU(2 * word_hidden_size, 2 * sent_hidden_size,bidirectional=True)\n",
    "         \n",
    "    def forward(self, word_attn_vectors, state_sent):\n",
    "        outputs, _ = self.gru(word_attn_vectors, state_sent)\n",
    "        outputs = (outputs[:, :, :self.sent_hidden_size] +\n",
    "                   outputs[:, :, self.sent_hidden_size:])\n",
    "        return outputs\n",
    "    \n",
    "class SentAttention(nn.Module):\n",
    "    def __init__(self, sent_gru_hidden):  \n",
    "        super().__init__()\n",
    "        self.weight_W_sent = nn.Parameter(torch.Tensor(2*sent_gru_hidden ,2*sent_gru_hidden))\n",
    "        self.bias_sent = nn.Parameter(torch.Tensor(2*sent_gru_hidden,1))\n",
    "        self.weight_proj_sent = nn.Parameter(torch.Tensor(2*sent_gru_hidden, 1))\n",
    "        self.softmax_sent = nn.Softmax()\n",
    "        self.weight_W_sent.data.uniform_(-0.1, 0.1)\n",
    "        self.weight_proj_sent.data.uniform_(-0.1,0.1)\n",
    "        self.sent_hidden_size = sent_hidden_size\n",
    "     \n",
    "    def forward(self, encoder_outputs):      \n",
    "        sent_squish = batch_matmul_bias(encoder_outputs, self.weight_W_sent,self.bias_sent, nonlinearity='tanh')\n",
    "        sent_attn = batch_matmul(sent_squish, self.weight_proj_sent)\n",
    "        sent_attn_norm = self.softmax_sent(sent_attn.transpose(1,0))\n",
    "        sent_attn_vectors = attention_mul(encoder_outputs, sent_attn_norm.transpose(1,0))   \n",
    "        return sent_attn_vectors, sent_attn_norm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadlineEncoderRNN(nn.Module):\n",
    "    def __init__(self, batch_size, hline_hidden_size, sent_hidden_size):\n",
    "        super().__init__()\n",
    "        self.hline_hidden_size = hline_hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.sent_hidden_size = sent_hidden_size\n",
    "        self.n_classes = n_classes\n",
    "        self.gru = nn.GRU(2 * sent_hidden_size, hline_hidden_size, bidirectional=True)\n",
    "\n",
    "    def forward(self, sent_attn_vectors, state_hline):\n",
    "        outputs, _ = self.gru(sent_attn_vectors, state_hline)\n",
    "        outputs = (outputs[:, :, :self.hline_hidden_size] +\n",
    "                   outputs[:, :, self.hline_hidden_size:])\n",
    "        return outputs \n",
    "\n",
    "\n",
    "class HlineAtention(nn.Module):\n",
    "    def __init__(self, hline_hidden_size):\n",
    "        super().__init__()\n",
    "        self.weight_W_hline = nn.Parameter(torch.Tensor(2*hline_hidden_size, 2*hline_hidden_size))\n",
    "        self.bias_hline = nn.Parameter(torch.Tensor(2*hline_hidden_size, 1))\n",
    "        self.weight_proj_hline = nn.Parameter(torch.Tensor(2*hline_hidden_size,1))\n",
    "        self.softmax_hline = nn.Softmax()\n",
    "        self.weight_W_hline.data.uniform(-0.1, 0.1)\n",
    "        self.weight_proj_hline.data.uniform(-0.1, 0.1)\n",
    "        self.hline_hidden_size = hline_hidden_size\n",
    "\n",
    "        # multi label classifier params\n",
    "        self.weight_W_fcc = nn.Parameter(torch.Tensor(4, 2*hline_hidden_size))\n",
    "        self.bias_fcc = nn.Parameter(torch.Tensor(4, 1))\n",
    "         \n",
    "    def forward(self, hline_attn_vectors, state_hline):\n",
    "        hline_squish = batch_matmul_bias(hline_attn_vectors, self.weight_W_hline,self.bias_hline, nonlinearity='tanh')\n",
    "        hline_attn = batch_matmul(hline_squish, self.weight_proj_hline)\n",
    "        hline_attn_norm = self.softmax_hline(hline_attn.transpose(1,0))\n",
    "        hline_attn_vectors = attention_mul(encoder_outputs, hline_attn_norm.transpose(1,0))  \n",
    "\n",
    "        # final multi-label prediction\n",
    "        final_label_vector = fcc_matmul(hline_attn_vectors, self.bias_fcc, self.weight_W_fcc)\n",
    "        return hline_attn_vectors, final_label_vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
